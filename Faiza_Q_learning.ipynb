{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Name: Faiza Siddiqui </center>\n",
    "### <center> 25-09-2021</center>\n",
    "### <center> Q-learning </center> \n",
    "\n",
    "## An introduction to Q-Learning: Reinforcement Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start, what is Q-Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As kids, you must have solved maze problems, in which an animal finds its way through the maze. In reinforcement learning also similar kinds of steps are going around, that is an agent will learn how to pick up the paths. \n",
    "### Q-learning is nothing but a reinforcement learning algorithm to learn the value of an action in a particular state. The 'Q' in Q-learning refers to the quality. So we always look for the optimal(best) path to reach our goal. Quality here represents how useful a given action is in gaining some future reward. \n",
    "### Q-Learning has 3 major components : State, Action & Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTp2zGHxl4g3C5w-lU5TnaiHvKysotpiwNrRIh8SViSXr1nohsh-lJINPqVXC1C7-0pxdQ&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State refers to the current position of an agent in an environmentâ€¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Action is the step taken by the agent when the agent is in a particular state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For every action, the agent will get a positive or negative reward. Reward function is, R: S X A â†’ R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In our example the environment is shown below, it is a guitar building factory with 9 different locations that agent can be in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/c8LXj7X/Capture.png)\n",
    "\n",
    "<center>The environment</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above environment, L6 is the end-goal of the agent, with top-priority location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The agents, in this case, are the robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### States are the locations in which a particular robot can be present at any given time. We have mapped the location codes to numbers, since that is easier for machines to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://blog.floydhub.com/content/images/2019/05/image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions are direct locations to go from the states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://blog.floydhub.com/content/images/2019/05/image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the basic understanding clear, let's start the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are defining the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_state = {\n",
    "    'L1' : 0,\n",
    "    'L2' : 1,\n",
    "    'L3' : 2,\n",
    "    'L4' : 3,\n",
    "    'L5' : 4,\n",
    "    'L6' : 5,\n",
    "    'L7' : 6,\n",
    "    'L8' : 7,\n",
    "    'L9' : 8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are now defining the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actions\n",
    "actions = [0,1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### While running the algorithm, there are multiple paths that an agent can take. We can find the best paths for the agent with the help of a table called Q-Table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/k4kgnQS/Capture.png)\n",
    "\n",
    "<center>Rewards' table</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the help of the table above we can now define the rewards matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rewards\n",
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "              [1,0,1,0,0,0,0,0,0],\n",
    "              [0,1,0,0,0,1,0,0,0],\n",
    "              [0,0,0,0,0,0,1,0,0],\n",
    "              [0,1,0,0,0,0,0,1,0],\n",
    "              [0,0,1,0,0,0,0,0,0],\n",
    "              [0,0,0,1,0,0,0,1,0],\n",
    "              [0,0,0,0,1,0,1,0,1],\n",
    "              [0,0,0,0,0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping indices to locations\n",
    "state_to_location = dict((state,location) for location, state in location_to_state.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is going to take two arguments: \n",
    "\n",
    "- starting location in the warehouse and \n",
    "- end location in the warehouse respectively \n",
    "\n",
    "It will return the optimal route for reaching the end location from the starting location in the form of an ordered list (containing the letters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before writing a helper function, let's learn about two parameters of Q-Learning algorithm - É‘ (learning rate) and ðœ¸ (discount factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha(É‘) is the learning rate which controls how quickly the robot adopts to the random changes imposed by the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initialize parameters\n",
    "\n",
    "gamma = 0.75 # Discount factor \n",
    "alpha = 0.9 # Learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The formula for Q-learning is given below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://randomant.net/the-algorithm-behind-the-curtain-understanding-how-machines-learn-with-q-learning/q_learning_algorithm_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is going to take two arguments: \n",
    "\n",
    "- starting location in the warehouse and \n",
    "- end location in the warehouse respectively \n",
    "\n",
    "It will return the optimal route for reaching the end location from the starting location in the form of an ordered list (containing the letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_route(start_location,end_location):\n",
    "    \n",
    "    # Copy the rewards matrix to new Matrix\n",
    "    \n",
    "    new_rewards = np.copy(rewards)\n",
    "    \n",
    "    # Get the ending state corresponding to the ending location as given\n",
    "    end_state = location_to_state[end_location]\n",
    "    \n",
    "    # With the above information automatically set the priority of the given ending state to the highest one\n",
    "    new_rewards[end_state, end_state] = 999\n",
    "\n",
    "    # -----------Q-Learning algorithm-----------\n",
    "   \n",
    "    # Initializing the Q-Table with all 0 values\n",
    "    Q = np.array(np.zeros([9,9]))\n",
    "\n",
    "    # Q-Learning process\n",
    "    for i in range(1000):\n",
    "        \n",
    "        # Pick up a state randomly\n",
    "        current_state = np.random.randint(0,9) \n",
    "        \n",
    "        # For traversing through the neighbor locations in the maze\n",
    "        workable_actions = []\n",
    "        \n",
    "        # Iterate through the new rewards matrix and get the actions > 0\n",
    "        for j in range(9):\n",
    "            if (new_rewards[current_state,j] > 0):\n",
    "                workable_actions.append(j)\n",
    "                \n",
    "        # Pick an action randomly from the list of playable actions leading us to the next state\n",
    "        next_state = np.random.choice(workable_actions)\n",
    "        \n",
    "        # Compute the temporal difference\n",
    "        # The action here exactly refers to going to the next state\n",
    "        # Temporal Difference is\n",
    "        TemporalDifference = new_rewards[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n",
    "        \n",
    "        # Update the Q-Value using the Bellman equation\n",
    "        Q[current_state,next_state] += alpha * TemporalDifference\n",
    "\n",
    "    # Initialize the optimal route with the starting location\n",
    "    route = [start_location]\n",
    "    \n",
    "    # We do not know about the next location yet, so initialize with the value of starting location\n",
    "    next_location = start_location\n",
    "    \n",
    "    # We don't know about the exact number of iterations needed to reach to the final location hence while loop will be a good choice for iteratiing\n",
    "    while(next_location != end_location):\n",
    "        \n",
    "        # Fetch the starting state\n",
    "        starting_state = location_to_state[start_location]\n",
    "        \n",
    "        # Fetch the highest Q-value pertaining to starting state\n",
    "        next_state = np.argmax(Q[starting_state,])\n",
    "        \n",
    "        # We got the index of the next state. But we need the corresponding letter. \n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        \n",
    "        # Update the starting location for the next iteration\n",
    "        start_location = next_location\n",
    "    \n",
    "    return route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L9', 'L8', 'L5', 'L2', 'L1']\n"
     ]
    }
   ],
   "source": [
    "print(get_optimal_route('L9', 'L1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L7', 'L8', 'L5', 'L2', 'L3']\n"
     ]
    }
   ],
   "source": [
    "print(get_optimal_route('L7', 'L3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybe8CY07zp7D"
   },
   "source": [
    "### Now imagine the robot is in another such guitar company. The picture of which is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq-QPfDnx4Fo"
   },
   "source": [
    "#### States\n",
    "States are all of the possible locations within the guitar company. black boxes are for storing, whereas other locations are for the agent to move around. The **green square** is our high priority area. \n",
    "The black and green squares are **terminal states**!\n",
    "\n",
    "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)\n",
    "\n",
    "The states can be shown with a matrix of 11 rows and same number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to Dr. Daniel Soper for this example of Q-learning. My intention to put this example here is to just show that I did understand this example as well and I also tried to look at many other examples.\n",
    "https://www.youtube.com/watch?v=__t2XRxXGxI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9AdpFVfy6ya9"
   },
   "outputs": [],
   "source": [
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 11\n",
    "environment_columns = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
    "#The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
    "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
    "#each state (see next cell for a description of possible actions). \n",
    "#The value of each (state, action) pair is initialized to 0.\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07gGSNz07xtP"
   },
   "source": [
    "#### Actions\n",
    "There are four available directions: Up, Down, Right, Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "z43QX_t080q3"
   },
   "outputs": [],
   "source": [
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X25vn4VKw2as"
   },
   "source": [
    "#### Rewards\n",
    "\n",
    "Negative rewards (i.e., **punishments**) are used for all states except the goal.\n",
    "\n",
    "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map-rewards.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a 2D numpy array to hold the rewards for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "GIJu7XsLXw62"
   },
   "outputs": [],
   "source": [
    "#The array contains 11 rows and 11 columns as discussed above, and each value is initialized to -100\n",
    "\n",
    "rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "rewards[0, 5] = 100. #set the reward for the packaging area (i.e., the goal) to 100\n",
    "\n",
    "#define aisle that is white boxes for rows 1 through 9 as shown in figure above\n",
    "aisles = {} #store locations in a dictionary\n",
    "aisles[1] = [i for i in range(1, 10)] \n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 8)]\n",
    "aisles[3].append(9)\n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range(11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### set the rewards for all white boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_index in range(1, 10):\n",
    "  for column_index in aisles[row_index]:\n",
    "    rewards[row_index, column_index] = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print rewards matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    }
   ],
   "source": [
    "for row in rewards:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFEor01iCCin"
   },
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function if the specified location is a terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "DnCfO5tVG0LJ"
   },
   "outputs": [],
   "source": [
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
    "  if rewards[current_row_index, current_column_index] == -1.:\n",
    "    return False\n",
    "  else:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that will choose a random, non-terminal starting location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  current_row_index = np.random.randint(environment_rows)\n",
    "  current_column_index = np.random.randint(environment_columns)\n",
    "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "  #(i.e., until the chosen state is a 'white square').\n",
    "  while is_terminal_state(current_row_index, current_column_index):\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_columns)\n",
    "  return current_row_index, current_column_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an algorithm that will choose which action to take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "  #then choose the most promising value from the Q-table for this state.\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else: #choose a random action\n",
    "    return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function that will get the next location based on the chosen action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function that will get the shortest path between any location within the warehouse that the robot is allowed to travel and the item packaging location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else: #if this is a 'legal' starting location\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      #get the best action to take\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      #move to the next location on the path, and add the new location to the list\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjl9niKEqONs"
   },
   "source": [
    "#### Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "3N5BB0m0JHIn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = get_starting_location()\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    \n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JqQfjYdfyBh"
   },
   "source": [
    "## Get Shortest Paths\n",
    "Now that the AI agent has been fully trained, we can see what it has learned by displaying the shortest path between any location in the warehouse where the robot is allowed to travel and the item packaging area.\n",
    "\n",
    "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)\n",
    "\n",
    "Run the code cell below to try a few different starting locations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "F1YO3mj_oS2J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 9], [2, 9], [1, 9], [1, 8], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
      "[[5, 0], [5, 1], [5, 2], [5, 3], [4, 3], [3, 3], [3, 2], [3, 1], [2, 1], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [0, 5]]\n",
      "[[9, 5], [9, 4], [9, 3], [8, 3], [7, 3], [7, 4], [7, 5], [6, 5], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n"
     ]
    }
   ],
   "source": [
    "#display a few shortest paths\n",
    "print(get_shortest_path(3, 9)) #starting at row 3, column 9\n",
    "print(get_shortest_path(5, 0)) #starting at row 5, column 0\n",
    "print(get_shortest_path(9, 5)) #starting at row 9, column 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWx7BsJxqrDv"
   },
   "source": [
    "#### Result:\n",
    "Agent can take the shortest path from any location in the environment.\n",
    "\n",
    "Let's also try reversing the order of the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "fKun8LInsas9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5], [1, 5], [1, 4], [1, 3], [1, 2], [1, 1], [2, 1], [3, 1], [3, 2], [3, 3], [4, 3], [5, 3], [5, 2]]\n"
     ]
    }
   ],
   "source": [
    "#display an example of reversed shortest path\n",
    "path = get_shortest_path(5, 2) #go to row 5, column 2\n",
    "path.reverse()\n",
    "print(path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
